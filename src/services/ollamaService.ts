interface OllamaResponse {
  response: string;
  done: boolean;
}

interface OllamaRequest {
  model: string;
  prompt: string;
  stream: boolean;
  options?: {
    temperature?: number;
    top_p?: number;
    num_predict?: number;
  };
}

class OllamaService {
  private baseUrl = 'http://localhost:11434';
  private defaultModel = 'llama2:7b-chat';
  private isConnected = false;

  async checkConnection(): Promise<boolean> {
    try {
      console.log('üîç Verificando conexi√≥n con Ollama...');
      const response = await fetch(`${this.baseUrl}/api/tags`, {
        method: 'GET',
        headers: {
          'Content-Type': 'application/json',
        },
      });
      
      if (response.ok) {
        this.isConnected = true;
        console.log('‚úÖ Ollama conectado correctamente');
        return true;
      } else {
        console.log('‚ùå Ollama respondi√≥ con error:', response.status);
      }
    } catch (error) {
      console.log('‚ùå Ollama no est√° disponible:', error);
      this.isConnected = false;
    }
    return false;
  }

  async getAvailableModels(): Promise<string[]> {
    try {
      const response = await fetch(`${this.baseUrl}/api/tags`);
      if (response.ok) {
        const data = await response.json();
        const models = data.models?.map((model: any) => model.name) || [];
        console.log('üìã Modelos disponibles:', models);
        return models;
      }
    } catch (error) {
      console.error('Error obteniendo modelos:', error);
    }
    return [];
  }

  async generateResponse(prompt: string, model?: string): Promise<string> {
    const modelToUse = model || this.defaultModel;
    
    try {
      console.log('ü§ñ Enviando prompt a Ollama...');
      console.log('üìù Modelo:', modelToUse);
      console.log('üìÑ Prompt (primeros 200 chars):', prompt.substring(0, 200) + '...');
      
      const request: OllamaRequest = {
        model: modelToUse,
        prompt: prompt,
        stream: false,
        options: {
          temperature: 0.3, // M√°s determin√≠stico para evaluaciones
          top_p: 0.9,
          num_predict: 800 // Limitar tokens para respuestas m√°s concisas
        }
      };

      const response = await fetch(`${this.baseUrl}/api/generate`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(request),
      });

      if (!response.ok) {
        throw new Error(`HTTP error! status: ${response.status}`);
      }

      const data: OllamaResponse = await response.json();
      console.log('‚úÖ Respuesta recibida de Ollama');
      console.log('üìÑ Respuesta (primeros 300 chars):', data.response.substring(0, 300) + '...');
      
      return data.response;
    } catch (error) {
      console.error('‚ùå Error llamando a Ollama:', error);
      throw error;
    }
  }

  getConnectionStatus(): boolean {
    return this.isConnected;
  }

  setModel(model: string): void {
    this.defaultModel = model;
    console.log('üîÑ Modelo cambiado a:', model);
  }

  getCurrentModel(): string {
    return this.defaultModel;
  }
}

export const ollamaService = new OllamaService();